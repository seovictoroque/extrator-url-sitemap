# Extrator de URL para sitemap

Este script em Python foi criado para extrair todas as URLs de um ou mais sitemaps e salv√°-las em um arquivo de texto ou XML. O principal objetivo do c√≥digo √© acessar o sitemap, coletar as URLs presentes nas tags <loc> e armazen√°-las em um arquivo para an√°lise futura. Essa funcionalidade √© particularmente √∫til em contextos como grandes e-commerces, onde √© essencial verificar e gerenciar um grande n√∫mero de p√°ginas listadas no sitemap, garantindo posteriormente que estejam funcionando corretamente e indexadas de maneira adequada.

## Funcionalidades

- Acessa m√∫ltiplos arquivos de sitemap paginados
- Extrai todas as URLs dos sitemaps
- Salva as URLs extra√≠das em um arquivo `.txt`
- Adiciona um delay para evitar bloqueios

---

## Como usar o Sitemap URL Extractor

### 1. Pr√©-requisitos

Antes de executar o script, certifique-se de ter o Python instalado. Se n√£o tiver o Python instalado, baixe-o [aqui](https://www.python.org/downloads/).

- **Python 3.6+**
- **Pip** (gerenciador de pacotes do Python)

### 2. Instalar depend√™ncias

### **Instalar o lxml**

No **VS Code**, abra o terminal e execute:

```bash
pip install lxml
```

Abra o terminal e execute:

```bash
pip install -r requirements.txt
```

> Se o arquivo requirements.txt n√£o existir, instale manualmente:
> 

```bash
pip install requests beautifulsoup4
```

### 3. Executar o script

Para rodar o script, use:

```bash
python extrair_sitemap.py
```

Isso processar√° todas as p√°ginas do sitemap e salvar√° as URLs em `all_sitemap_urls.txt`.

---

## <C√≥digo_v0.1>
Neste c√≥digo, o script segue um padr√£o de URL de sitemap no formato https://www.site.com.br/sitemap/products/1 at√© https://www.site.com.br/sitemap/products/190. No entanto, caso o padr√£o de URLs das PDPs (P√°ginas de Produto) no sitemap do seu site seja diferente, ser√° necess√°rio ajustar o c√≥digo para adaptar o looping √† estrutura espec√≠fica das URLs. Como alternativa, voc√™ pode utilizar a vers√£o 0.2 do c√≥digo, dispon√≠vel logo abaixo, ideal para extrair listar por lista.

```python
# Bibliotecas
import requests
from bs4 import BeautifulSoup
import time  

# Fun√ß√£o para extrair URLs de uma p√°gina do sitemap, altere a URL para o site que deseja
def extract_urls_from_sitemap(page_number):
    sitemap_url = f"https://www.site.com.br/sitemap/products/{page_number}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }

    try:
        # Fazendo a requisi√ß√£o HTTP com o cabe√ßalho
        response = requests.get(sitemap_url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "xml")
            urls = [url.text for url in soup.find_all("loc")]
            return urls
        else:
            print(f"Erro ao acessar o sitemap {page_number}: {response.status_code}")
            return []
    except requests.RequestException as e:
        print(f"Erro na requisi√ß√£o do sitemap {page_number}: {e}")
        return []

# Fun√ß√£o principal
def main():
    total_pages = 190
    all_urls = []

    # Loop para processar todas as listas do sitemap
    for page in range(1, total_pages + 1):
        print(f"Processando sitemap {page}...")
        urls = extract_urls_from_sitemap(page)
        if urls:  # Verifica se a lista de URLs n√£o est√° vazia
            all_urls.extend(urls)
            print(f"Extra√≠das {len(urls)} URLs da p√°gina {page}")
        else:
            print(f"Nenhuma URL encontrada na p√°gina {page} ou erro ao acessar.")

        # Delay de 1 segundo entre as requisi√ß√µes
        time.sleep(1)

    # Salva todas as URLs em um √∫nico arquivo
    # Altere "all_sitemap_urls.txt" pelo nome que deseja
    with open("all_sitemap_urls.txt", "w") as f:
        for url in all_urls:
            f.write(url + "\n")

    print(f"Total de URLs extra√≠das: {len(all_urls)}. Salvas no arquivo all_sitemap_urls.txt")

if __name__ == "__main__":
    main()
```
## <C√≥digo_v0.2>

Nesta vers√£o do c√≥digo, o script acessar√° o sitemap especificado e extrair√° as URLs exclusivamente dessa lista de sitemap.

```python
import requests
from bs4 import BeautifulSoup

# URL do Sitemap
sitemap_url = "https://www.site.com.br/sitemap/products/1"

# Cabe√ßalho para simular um navegador real
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# Fazendo a requisi√ß√£o HTTP com o cabe√ßalho
response = requests.get(sitemap_url, headers=headers)

# Se a requisi√ß√£o foi bem-sucedida
if response.status_code == 200:
    soup = BeautifulSoup(response.content, "xml")
    urls = [url.text for url in soup.find_all("loc")]

    # Salvando em um arquivo
    with open("sitemap_urls.txt", "w") as f:
        for url in urls:
            f.write(url + "\n")

    print(f"Extra√≠das {len(urls)} URLs e salvas no arquivo sitemap_urls.txt")
else:
    print(f"Erro ao acessar o sitemap: {response.status_code}")

```
---

## Estrutura do Projeto

```
üìÅ extrator_sitemap/
‚îÇ‚îÄ‚îÄ extrair_sitemap.py   # Script principal
‚îÇ‚îÄ‚îÄ requirements.txt     # Depend√™ncias do projeto
‚îÇ‚îÄ‚îÄ all_sitemap_urls.txt # Sa√≠da com as URLs extra√≠das
‚îÇ‚îÄ‚îÄ README.md            # Documenta√ß√£o
```

---

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Para contribuir:

1. Fa√ßa um **fork** do reposit√≥rio
2. Crie um **branch** (`git checkout -b minha-feature`)
3. Fa√ßa commit das mudan√ßas (`git commit -m 'Adicionando nova feature'`)
4. Fa√ßa push para o branch (`git push origin minha-feature`)
5. Abra um **Pull Request**

---

## Licen√ßa

Este projeto √© licenciado sob a **MIT License**. Sinta-se livre para utiliz√°-lo e modific√°-lo. 

---

## Contato

Se tiver d√∫vidas ou sugest√µes, entre em contato:

- **E-mail:** seovictoroque@email.com
- X (twitter) **:** [@seovictor](https://x.com/seovictoroque)
